import os
import time

import streamlit as st
from embeddings_generator import load_vector_store
from rag_chatbot import generate_rag_response, load_prompts
from utils import get_openai_client

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ RAG ì±—ë´‡",
    page_icon="ğŸ’¬",
    layout="centered",
)

# ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown(
    """
<style>
.chat-message {
    padding: 1.5rem; 
    border-radius: 0.5rem; 
    margin-bottom: 1rem; 
    display: flex;
    flex-direction: row;
    align-items: flex-start;
}
.chat-message.user {
    background-color: #F0F2F6;
}
.chat-message.bot {
    background-color: #E8F4FE;
}
.chat-message .avatar {
    width: 40px;
    height: 40px;
    border-radius: 50%;
    object-fit: cover;
    margin-right: 1rem;
}
.chat-message .message {
    flex: 1;
}
.stTextInput {
    position: fixed;
    bottom: 3rem;
    background-color: white;
    left: 0;
    right: 0;
    padding: 1rem 5rem;
}
</style>
""",
    unsafe_allow_html=True,
)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ í™•ì¸
try:
    prompts = load_prompts()
    if "system_prompts" in prompts and "rag" in prompts["system_prompts"]:
        st.session_state.prompts_loaded = True
    else:
        st.session_state.prompts_loaded = False
except Exception as e:
    st.error(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.session_state.prompts_loaded = False

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_store" not in st.session_state:
    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    try:
        vector_store = load_vector_store()
        if vector_store:
            st.session_state.vector_store = vector_store
            st.session_state.vector_store_loaded = True
        else:
            st.session_state.vector_store_loaded = False
    except Exception as e:
        st.error(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.session_state.vector_store_loaded = False

# ì œëª© ë° ì†Œê°œ
st.title("í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ RAG ì±—ë´‡")

# ë¡œë“œ ìƒíƒœ í‘œì‹œ
status_messages = []

if not st.session_state.get("prompts_loaded", False):
    status_messages.append("âš ï¸ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: 'code/prompts.yaml' íŒŒì¼ í™•ì¸ í•„ìš”")
else:
    status_messages.append("âœ… í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ")

if not st.session_state.get("vector_store_loaded", False):
    status_messages.append(
        "âš ï¸ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: 'vector_store.json' íŒŒì¼ í™•ì¸ í•„ìš”"
    )
else:
    status_messages.append(
        f"âœ… ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì„±ê³µ: {len(st.session_state.vector_store)}ê°œ ì²­í¬"
    )

if status_messages:
    status_text = "\n".join(status_messages)
    if "âš ï¸" in status_text:
        st.warning(status_text)
    else:
        st.success(status_text)

# í•„ìˆ˜ êµ¬ì„± ìš”ì†Œê°€ ëª¨ë‘ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
can_respond = st.session_state.get(
    "vector_store_loaded", False
) and st.session_state.get("prompts_loaded", False)
if not can_respond:
    st.error("ì¼ë¶€ êµ¬ì„± ìš”ì†Œê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ì±—ë´‡ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ìƒˆ ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)

    # ë´‡ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        if can_respond:
            try:
                # ì‹¤ì œ ì‘ë‹µ ìƒì„±
                response = generate_rag_response(prompt, st.session_state.vector_store)

                # íƒ€ì´í•‘ íš¨ê³¼
                for chunk in response.split():
                    full_response += chunk + " "
                    message_placeholder.markdown(full_response + "â–Œ")
                    time.sleep(0.01)

                message_placeholder.markdown(full_response)

                # ì‘ë‹µ ì €ì¥
                st.session_state.messages.append(
                    {"role": "assistant", "content": full_response}
                )
            except Exception as e:
                error_msg = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                message_placeholder.markdown(error_msg)
                st.session_state.messages.append(
                    {"role": "assistant", "content": error_msg}
                )
        else:
            error_msg = "í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            message_placeholder.markdown(error_msg)
            st.session_state.messages.append(
                {"role": "assistant", "content": error_msg}
            )

# ì‚¬ì´ë“œë°” ì •ë³´
with st.sidebar:
    st.header("ì •ë³´")
    st.markdown(
        """
    ì´ ì±—ë´‡ì€ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG(Retrieval-Augmented Generation) ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ 
    ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    
    **ì£¼ìš” ê¸°ëŠ¥:**
    - ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ê¸°ë°˜ ì§€ì‹ í™œìš©
    - ì„ë² ë”© ë²¡í„°ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
    - OpenAI APIë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±
    """
    )

    st.header("ì‚¬ìš© ë°©ë²•")
    st.markdown(
        """
    ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”:
    - ì»´í“¨í„°ê³µí•™ë¶€ ì¡¸ì—… ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?
    - í•™ê³¼ì˜ ì£¼ìš” êµê³¼ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?
    - ëŒ€í•™ì› ì…í•™ ìš”ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
    """
    )

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# ì•± ì‹¤í–‰ ëª…ë ¹ì–´: streamlit run code/web_ui.py
